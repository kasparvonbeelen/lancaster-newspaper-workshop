{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasparvonbeelen/lancaster-newspaper-workshop/blob/wc/word_counts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data/Culture Workshop, Lancaster\n",
        "## Content Analysis of Historical Newspapers\n"
      ],
      "metadata": {
        "id": "TagovOvPfZbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's just turn of\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "rlKKE6oRPDbo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR quality\n",
        "\n",
        "In this notebook, we have a closer look at exploring newspaper content. But before we do that, let's have a look at the quality of the text data.\n",
        "\n",
        "A major hurdle for analysing the historical press, is the sometimes awful quality of the automatic text transcription, using Optical Character Recognition (OCR) software, which converts images to machine-readable text.\n",
        "\n",
        "And here ```m4n y th**in^gs can go wrong!```\n",
        "\n",
        "So before analysing/reading our sources, we should determine what is readable and how data quality might impact our findings.\n",
        "\n",
        "In this notebook, we investigate if the OCR errors are truly randomly distributed or more skewed towards certain categories of newspapers. This could inform how we read our findings later on."
      ],
      "metadata": {
        "id": "QavOGMu0O3J8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T9xo1rc0cYEw"
      },
      "outputs": [],
      "source": [
        "# We need to import the pandas library for working with spreadsheet\n",
        "import pandas as pd\n",
        "import re # another library for matching patterns in text\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import seaborn as sns # import seaborn for making plots a bit prettier\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataframe from github\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/kasparvonbeelen/lancaster-newspaper-workshop/wc/data/subsample500mixedocr-selected_mitch.csv\")\n",
        "# for convenience we drop rows that have nan (not a number values)\n",
        "# otherwise some of the scripts and operation might crash\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "QocF9iPZd4qZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print the first n-rows to get a sense of the information available to us."
      ],
      "metadata": {
        "id": "z4p76cdQCV7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "V9_6lOM2k8YB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter plots\n",
        "\n",
        "The most 'direct' way to interrogate data is to look at scatterplots."
      ],
      "metadata": {
        "id": "fDbSS160B-Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df,\n",
        "                 x=\"word_count\",\n",
        "                 y=\"ocrquality\",\n",
        "                 color=\"political_leaning_label\",\n",
        "                 hover_data=['date','newspaper_title',\"political_leaning_label\", \"price_label\"],\n",
        "                 trendline_scope=\"overall\",\n",
        "                 trendline=\"ols\",\n",
        "                 width=1000, height=500,\n",
        "                 )\n",
        "fig.update_layout(showlegend=True)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Wf_ObKJ1O2n6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log-scale\n",
        "\n",
        "\n",
        "A common technique to declutter the visualisation is to use a log-scale, this will make a small difference bigger and a big difference smaller."
      ],
      "metadata": {
        "id": "Jf_qnwhICj4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.log([1, 5]))\n",
        "print(np.log([100,1000]))"
      ],
      "metadata": {
        "id": "N_SATH2lEiE6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[(df.ocrquality > 0) & (df.word_count > 0)]\n",
        "df['word_count_log'] = np.log(df['word_count'] )\n",
        "fig = px.scatter(df,\n",
        "                 x=\"word_count_log\",\n",
        "                 y=\"ocrquality\",\n",
        "                 color=\"political_leaning_label\",\n",
        "                 hover_data=['date','newspaper_title',\"political_leaning_label\", \"price_label\"],\n",
        "                 trendline_scope=\"overall\",\n",
        "                 trendline=\"ols\",\n",
        "                 width=1000, height=500,\n",
        "                 )\n",
        "fig.update_layout(showlegend=True)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "T3ZerVkSPxsI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Plot the OCR quality over time using a scatter plot."
      ],
      "metadata": {
        "id": "U2DxcVuWHjH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter code here"
      ],
      "metadata": {
        "id": "Tx6bmZ0ZE48-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other plotting options\n",
        "\n",
        "We can visualize distributions as histograms or density plots."
      ],
      "metadata": {
        "id": "hR7dT5rMIBRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.political_leaning_label.isin(['conservative','liberal'])].groupby(['political_leaning_label'])['ocrquality'].plot(kind='hist', bins=100, alpha=.6)"
      ],
      "metadata": {
        "id": "SuRzaCPxIKab"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.political_leaning_label.isin(['conservative','liberal'])].groupby(['political_leaning_label'])['ocrquality'].plot(kind='density')"
      ],
      "metadata": {
        "id": "KmWjIPvXQWNQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Is the OCR of the halfpenny press (½ d) worse than the papers priced at 1d? For the exercise, ignore all other newspapers outside of these price points."
      ],
      "metadata": {
        "id": "-2HxcoqxS1ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.price_label.value_counts()"
      ],
      "metadata": {
        "id": "aQ9Z9C7Xd8KM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enter your answer here, adapt the previous line of code df[df.political_leaning_label."
      ],
      "metadata": {
        "id": "jtKRSsw1dahx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Content Analysis"
      ],
      "metadata": {
        "id": "OcgCfYq2PKUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Counting Words with Regular Expressions"
      ],
      "metadata": {
        "id": "0tntDECfhlvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular expressions offer a convenient tool to explore content by searching and investigating the occurrence of specific patterns in the corpus.\n",
        "\n",
        "Below we construct a regular expression in which we aim to capture multiple words (and variants) at once.\n",
        "\n",
        "In abstract terms, the regex follows the format:\n",
        "`\"\\b(query_1|query_2|...|query_n)\\b\"`\n",
        "\n",
        "- `\\b` indicates a word break, which can be a white space or interpunction symbol\n",
        "- `|` indicates OR, i.e. we want to find any of the queried items\n",
        "- `s?` ensures we include plural forms\n",
        "\n",
        "In Python, we first formulate the regex as a 'raw' string (a string prefixed by `r`' and then compile it, where add extra flags, in this `re.I` ignoring the difference between upper and lower case)"
      ],
      "metadata": {
        "id": "fv5mAhYui0vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the regular expression\n",
        "regex = r\"\\b(trains?|rails?)\\b\"\n",
        "# compile the regex use an ignore case flag\n",
        "# i.e. we will ignore uppercase\n",
        "pattern = re.compile(regex, re.I)"
      ],
      "metadata": {
        "id": "U4DD4cCkhufb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the regex on a particular example\n",
        "example_text = 'I took to trAin from Euston to Lancaster, but thetrain was delayed because there were leaves on the rails!'\n",
        "pattern.findall(example_text)"
      ],
      "metadata": {
        "id": "Ax7vTku5jsxH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of applying the regex to one example, we can apply it to all items in the `text` column of our dataframe. For this, we need to apply the `.apply` method (what's in a name!) to the text column.\n",
        "\n",
        "What does this operation return? For each row, it will return words that match our query regex, or return an empty list (or `[]`) in case we do not find anything!\n",
        "\n",
        "In the code cell below, we apply the regex to all items in our dataframe."
      ],
      "metadata": {
        "id": "DJoPSmnTkPNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'].apply(pattern.findall)"
      ],
      "metadata": {
        "id": "1rOoCwJOfLMs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we want to store the result of the `pattern.findall` operation and add the query results as a new column to the dataframe. In `pandas` this is relatively straightforward and resembles the variable assignment operation.\n",
        "\n",
        "After saving the results in a new column, we can keep track of the number of matched items in the text (and the corpus). These results are stored in the `num_hits` column."
      ],
      "metadata": {
        "id": "xXwMmKC2nWwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['hits'] = df['text'].apply(pattern.findall) # safe the query results in a new column\n",
        "df['num_hits'] = df['hits'].apply(len) # count the number of items found"
      ],
      "metadata": {
        "id": "BhTSSXupfXDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_hits'].value_counts() # get the distribution of hits"
      ],
      "metadata": {
        "id": "kWehtDs_lnys"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_hits'].value_counts().plot(kind='bar') # plot the distribution as a bar chart"
      ],
      "metadata": {
        "id": "85zJpk30ltVJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect the result of the `findall` operation more closely, and zoom in on the examples where we encounter more than one hit. We use `df.num_hits > 0` as a filter to select only rows with contain at least one mention of 'train' or 'rails'."
      ],
      "metadata": {
        "id": "pO_AWMpBpjPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_hits = df[df.num_hits > 0].reset_index()\n",
        "df_with_hits[['hits','text']]"
      ],
      "metadata": {
        "id": "-SX3kIYWl4_6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We print the full content of the 4th text."
      ],
      "metadata": {
        "id": "vasIl0p0Rarh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_with_hits.iloc[4][['hits','text']].text)"
      ],
      "metadata": {
        "id": "NX_W9DrJrZ5L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Print the content of the 7th text."
      ],
      "metadata": {
        "id": "ZycKdYVJRTsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter code here"
      ],
      "metadata": {
        "id": "Zg5_NE-uVf4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Search the newspaper dataframe for two (or more!) words of choice."
      ],
      "metadata": {
        "id": "yeX4UzJmprxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy version\n",
        "\n",
        "Select query terms and see how often these appear in the corpus."
      ],
      "metadata": {
        "id": "WAKH5edep1a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_1 = '' # add a query term between the quotation marks\n",
        "query_2 = '' # add a query term between the quotation marks\n",
        "\n",
        "regex = rf\"\\b({query_1}|{query_2})\\b\"\n",
        "# compile the regex use an ignore case flag\n",
        "# i.e. we will ignore uppercase\n",
        "pattern = re.compile(regex, re.I) # compile\n",
        "df['hits'] = df['text'].apply(pattern.findall) # safe the query results in a new column\n",
        "df['num_hits'] = df['hits'].apply(len) # count the number of items found\n",
        "df['num_hits'].value_counts()"
      ],
      "metadata": {
        "id": "C90Pj_tDp3wg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advanced version\n",
        "\n",
        "- Define a new regular expression that queries the corpus for at least 2 words.\n",
        "- Look at the previous examples and adapt the code to plot the distribution of the hits.\n"
      ],
      "metadata": {
        "id": "kLfP54Xkp6qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = ''\n",
        "pattern = re.compile(regex, re.I) # compile\n",
        "df['hits'] = df['text'].apply\n",
        "= df['hits'].apply(len)\n",
        "# plot the distribution of hits"
      ],
      "metadata": {
        "id": "APGY6rlGp_Rd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text and Metadata\n",
        "\n",
        "Simply counting how often certain items appear is not that interesting. To use newspaper archives for making historical arguments, we often rely on metadata. More precisely, studying the relation between metadata and full-text content is where things get interesting historically.\n",
        "\n",
        "Below we have a closer (and practical) look at some examples.\n"
      ],
      "metadata": {
        "id": "VdDCaO5Fs0Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below repeats the regex-based search operations we discussed previously."
      ],
      "metadata": {
        "id": "HHmc-lNvW71k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/kasparvonbeelen/lancaster-newspaper-workshop/wc/data/subsample500mixedocr-selected_mitch.csv\")\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# define the regular expression\n",
        "regex = r\"\\b(trains?|rails?)\\b\"\n",
        "# compile the regex use an ignore case flag\n",
        "# i.e. we will ignore uppercase\n",
        "pattern = re.compile(regex, re.I)\n",
        "df['hits'] = df['text'].apply(pattern.findall) # safe the query results in a new column\n",
        "df['num_hits'] = df['hits'].apply(len) # count the number of items found\n"
      ],
      "metadata": {
        "id": "mER8GFBtDmwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timelines\n",
        "\n",
        "We group the results by year and count how often we encounter 'trains' in historical newspapers over the nineteenth century.\n",
        "\n",
        "### Questions\n",
        "\n",
        "- What is shown in the timeline below?\n",
        "- And how could it be misleading?"
      ],
      "metadata": {
        "id": "wzQ2tld9WdL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('year')['num_hits'].count().plot()"
      ],
      "metadata": {
        "id": "QaCn5wxGs288"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To investigate change over time, we need to 'normalize' results by year, to make the results comparable. One way of doing this is to divide the number of hits, by the total number of words.\n",
        "\n",
        "As seen previously, we can use `.split()` to divide the string by white spaces, and then count the number of 'words'*\n",
        "\n",
        "*or a proxy to the number of words."
      ],
      "metadata": {
        "id": "mjzgZUjRXwqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "an_example_text = \"This sentence has 5 words.\"\n",
        "words = an_example_text.split()\n",
        "print(words)\n",
        "num_words = len(words)\n",
        "print(num_words)"
      ],
      "metadata": {
        "id": "uC7LeGbJs8cA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This sentence has 5 words.\"\n",
        "len(sentence.split())"
      ],
      "metadata": {
        "id": "KBEknO4BtdIt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we inspect the distribution of the document lengths using a histogram..."
      ],
      "metadata": {
        "id": "gj0VyKWpY3lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_words'] = df['text'].apply(lambda x: len(x.split()))\n",
        "df['num_words'].plot(kind='hist')"
      ],
      "metadata": {
        "id": "rg8yCZBJtk5T"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... Or plot the number of words by year."
      ],
      "metadata": {
        "id": "de-vB852Y_UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('year')['num_words'].sum().plot()"
      ],
      "metadata": {
        "id": "dD_lHPyQwXsx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use these total counts to plot a timeline that shows the prevalence of a topic while accounting for the changes in corpus size.\n",
        "\n",
        "To do this, we sum the number of hits and divide this by the total number of words for each year."
      ],
      "metadata": {
        "id": "wUBd0Np-vYmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped = df.groupby('year').apply(lambda x: x['num_hits'].sum() / x['num_words'].sum())\n",
        "df_grouped.plot()"
      ],
      "metadata": {
        "id": "qPbM-s47uEGB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What might be confusing about the plot is that they (kind of) point in different directions. Should we believe the 'raw' counts or the normalized frequencies?\n",
        "\n",
        "The short answer is that both are not reliable. Even though we imagine observing trends, we don't have enough data in this case to make any claims about historical change.\n",
        "\n",
        "Why do I think this is the case?"
      ],
      "metadata": {
        "id": "nofKXpB9aMLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the relative number of hits with confidence intervals (using the `seaborn` library)."
      ],
      "metadata": {
        "id": "_AvbOemEb4H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['ratio'] = df['num_hits'] /  df['num_words']\n",
        "sns.lineplot(x='year',y='ratio', data=df)"
      ],
      "metadata": {
        "id": "Mjv1ZSqxvjtT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermezzo: Understanding confidence intervals"
      ],
      "metadata": {
        "id": "uLUhlU5hceaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle, random\n",
        "scores = [random() for _ in range(100)]\n",
        "scores[:3]"
      ],
      "metadata": {
        "id": "hACiox1Knfry"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))"
      ],
      "metadata": {
        "id": "nc5ZaY3edM97"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 20\n",
        "n_trials = 100\n",
        "means = []\n",
        "for _ in range(n_trials):\n",
        "  shuffle(scores)\n",
        "  means.append(sum(scores[:size]) / size)\n",
        "ax = pd.Series(means).plot(kind='density')\n",
        "ax.axvline(sum(scores)/len(scores), color=\"black\", linestyle=\"dashed\")"
      ],
      "metadata": {
        "id": "H8fnYKZooxDb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentiles = np.percentile(means,q=[5.,95.])\n",
        "ax = pd.Series(means).plot(kind='density')\n",
        "ax.axvline(sum(scores)/len(scores), color=\"black\", linestyle=\"dashed\")\n",
        "ax.axvline(percentiles[0], color=\"red\", linestyle=\"dashed\")\n",
        "ax.axvline(percentiles[1], color=\"red\", linestyle=\"dashed\")"
      ],
      "metadata": {
        "id": "wRGKaEqbq3Fz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Politics\n",
        "\n"
      ],
      "metadata": {
        "id": "DZTKmCGklRWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.political_leaning_label.unique()"
      ],
      "metadata": {
        "id": "3HKsXiI8lT4e"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guided Exercise: Politics and Language\n",
        "\n",
        "- Create a simplified schema of these political labels that map each of the categories to either 'left', 'right' or 'non-aligned'.\n",
        "- Save the simplified labels in a new column `political_labels_simplified`.\n",
        "- Print the number of hits by political party using the simplified schema.\n",
        "- Visualize the results using a barplot."
      ],
      "metadata": {
        "id": "bkE6Y6lPl0hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Enter code here\n",
        "\n",
        "## create a mapping\n",
        "\n",
        "## mapping = {'liberal': ...}\n",
        "\n",
        "## apply mapping df[''].replace\n",
        "\n",
        "## use value_counts() to see the number of hits by party\n",
        "sns.barplot(x='political_labels_simplified',y='ratio', data=df)\n",
        "plt.xticks(rotation=70)"
      ],
      "metadata": {
        "id": "3-TsBSfqlzMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution\n",
        "\n",
        "Uncomment the code below."
      ],
      "metadata": {
        "id": "6CaAsnvseWat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping = {'liberal': 'left',\n",
        "#            'independent': 'non-aligned',\n",
        "#            'neutral': 'non-aligned',\n",
        "#            'constitutional': 'right',\n",
        "#             'liberal; conservative': 'non-aligned',\n",
        "#             'unionist':'right',\n",
        "#             'independent; conservative': 'right',\n",
        "#             'conservative':'right'}"
      ],
      "metadata": {
        "id": "mwREc27ulXEx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['political_labels_simplified'] = df['political_leaning_label'].replace(mapping)\n",
        "# df['political_labels_simplified'].value_counts()"
      ],
      "metadata": {
        "id": "EYKRkEucrzMB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.barplot(x='political_labels_simplified',y='ratio', data=df)\n",
        "# plt.xticks(rotation=70)"
      ],
      "metadata": {
        "id": "EHUk6NmEsWkg",
        "outputId": "a3410e41-05c1-400c-e115-25ab613007c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "political_labels_simplified\n",
              "non-aligned    163\n",
              "left           151\n",
              "right          113\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guided Example: Exploring the periodicity in newspapers\n",
        "\n",
        "Let's now play with a larger dataset and tie together everything we've seen so far. Instead of looking at change over time, we will inspect periodicities in historical newspapers."
      ],
      "metadata": {
        "id": "nSakFsMTszjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we download a larger sample of newspaper data\n",
        "# with approx 10_000 articles per year\n",
        "!wget -q --show-progress https://github.com/kasparvonbeelen/lancaster-newspaper-workshop/raw/wc/data/sample_lwm_hmd_mt90_10000.csv.zip"
      ],
      "metadata": {
        "id": "3urL5falsxtN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the downloaded sample\n",
        "!unzip sample_lwm_hmd_mt90_10000.csv.zip\n",
        "!rm -r __MACOSX"
      ],
      "metadata": {
        "id": "f4PvGR4nwvPZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "CoXQSAJPjPi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the csv file\n",
        "df_large = pd.read_csv('sample_lwm_hmd_mt90_10000.csv')"
      ],
      "metadata": {
        "id": "0hwdkVCyglU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the OCR quality by year\n",
        "sns.lineplot(x='year',y='ocrquality',data=df_large)"
      ],
      "metadata": {
        "id": "23i1khCYg4KU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the OCR quality by month\n",
        "sns.lineplot(x='month',y='ocrquality',data=df_large)"
      ],
      "metadata": {
        "id": "GWnSb4ZAjBLw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question\n",
        "\n",
        "Is there a significant change in OCR quality over the nineteenth century but not by month?"
      ],
      "metadata": {
        "id": "DXL73kDmgqFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the number of words for each document\n",
        "df_large['num_words'] = df_large.text.apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "APH9SI8UjTtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the average document length\n",
        "sns.lineplot(x='year',y='num_words',data=df_large)"
      ],
      "metadata": {
        "id": "JezWf22CjiWX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the average document length by month\n",
        "sns.lineplot(x='month',y='num_words',data=df_large)"
      ],
      "metadata": {
        "id": "jDjrW0Khj3jo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search the corpus using a particular regular expression\n",
        "tqdm.pandas() # use tqdm to print a progress bar\n",
        "#pattern = re.compile(r'\\btoo cold\\b', re.I)\n",
        "#pattern = re.compile(r'\\btoo hot\\b', re.I)\n",
        "pattern = re.compile(r'\\bcricket\\b', re.I) # create and compile a regex pattern\n",
        "df_large['matches'] = df_large.text.progress_apply(lambda x: pattern.findall(x)) # apply compile regular expression\n",
        "df_large['num_matches'] = df_large.matches.apply(len) # count number of hits for each document\n",
        "df_large['matches_ratio'] = df_large['num_matches'] / df_large['num_words'] # compute the ratio of hits\n",
        "sns.barplot(x='month',y='matches_ratio',data=df_large) # plot the results with error bars"
      ],
      "metadata": {
        "id": "eC8BfFJmjn17"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fin."
      ],
      "metadata": {
        "id": "_9MBws2GheHJ"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6CaAsnvseWat"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}